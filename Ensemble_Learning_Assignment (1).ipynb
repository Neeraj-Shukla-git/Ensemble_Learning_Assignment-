{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning | Assignment\n",
        "\n",
        "## Assignment Code: DA-AG-014\n",
        "\n",
        "### Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "- Ensemble Learning is a technique where multiple learning models (called base models or weak learners) are combined to produce a single stronger model. The core idea is that a group of models working together will perform better and be more robust than any single model alone — similar to “wisdom of the crowd.”\n",
        "- Key ideas:\n",
        "  - Combine multiple models to reduce variance, bias, or improve predictions.\n",
        "  - Models can be of same type (e.g., many decision trees) or different types (e.g., SVM + Logistic + Tree).\n",
        "  - Aggregation methods: voting (classification), averaging (regression), weighted voting, or stacking (meta-learner).\n",
        "  - Ensemble reduces overfitting (bagging) or reduces bias (boosting) depending on approach.\n",
        "- Example: Random Forest combines many decision trees trained on bootstrapped samples and averages/majority-votes their outputs.\n",
        "\n",
        "\n",
        "\n",
        "### Question 2: What is the difference between Bagging and Boosting?\n",
        "- Bagging (Bootstrap Aggregating):\n",
        "  - Purpose: Reduce variance and avoid overfitting.\n",
        "  - How: Train multiple base learners independently on different bootstrap samples (random sampling with replacement) of the training data, then aggregate (majority vote or average).\n",
        "  - Base learners: typically deep trees (unpruned).\n",
        "  - Example: Random Forest (bagging + random feature selection).\n",
        "\n",
        "- Boosting:\n",
        " - Purpose: Reduce bias (and sometimes variance) by sequentially training weak learners where each new learner focuses on examples previous ones mispredicted.\n",
        " - How: Train learners sequentially; each learner tries to correct mistakes of the ensemble so far; predictions combined via weighted sum/vote.\n",
        " - Algorithms: AdaBoost, Gradient Boosting Machines (GBM), XGBoost, LightGBM, CatBoost.\n",
        " - Boosting often gives higher accuracy than bagging but can be more prone to overfitting (needs regularization).\n",
        "\n",
        "| Aspect   |                Bagging | Boosting                           |\n",
        "| -------- | ---------------------: | ---------------------------------- |\n",
        "| Training | Parallel (independent) | Sequential                         |\n",
        "| Samples  |     Bootstrap (random) | Weighted focusing on hard examples |\n",
        "| Reduces  |               Variance | Bias (and sometimes variance)      |\n",
        "| Example  |          Random Forest | AdaBoost, XGBoost                  |\n",
        "\n",
        "\n",
        "### Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "- Bootstrap sampling: sampling technique where we draw samples with replacement from the original dataset to create many different training sets (each the same size as the original). Some original instances may repeat in a bootstrap sample; some may be left out.\n",
        "- Role in Bagging / Random Forest:\n",
        " - Creates diverse training subsets so base models (trees) see different data — increases model variety and reduces correlation between base learners.\n",
        " - Because base models are trained on different samples, averaging their predictions reduces variance and improves generalization.\n",
        " - In Random Forest, extra randomness (random feature selection at splits) + bootstrap sampling increases diversity among trees and boosts ensemble performance.\n",
        "\n",
        "### Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "- Out-of-Bag (OOB) samples: For each bootstrap sample, about ~37% of the original training instances are expected not to be included (because sampling with replacement leaves some items out). These left-out instances are OOB samples for that particular base model.\n",
        "- OOB score usage:\n",
        " - We can evaluate each sample using only the base learners that did not see it during training (i.e., those where it was OOB). Aggregating predictions across those learners gives an OOB-predicted label for that sample.\n",
        " - OOB score approximates cross-validation performance without a separate hold-out set, useful for Random Forests and bagging methods.\n",
        " - Provides internal validation estimate and can be used for model selection / early stopping in some implementations.\n",
        "\n",
        "### Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "- Single Decision Tree:\n",
        "  - Feature importance often measured by the total reduction of impurity (e.g., Gini or entropy) contributed by splits using that feature.\n",
        "  - Can be unstable — small data changes can change the tree structure and importances.\n",
        "  - Single tree may overemphasize features that fit training idiosyncrasies.\n",
        "\n",
        "- Random Forest:\n",
        "  - Feature importance averaged over many trees → more stable and reliable.\n",
        "\n",
        "- Two common measures:\n",
        " - Mean decrease in impurity (MDI): average impurity reduction across trees when splitting on feature.\n",
        " - Permutation importance (mean decrease accuracy): measure drop in performance when feature values are shuffled — gives model-agnostic, more trustworthy view.\n",
        "- Random Forest importances reduce bias from single-tree overfitting and account for interactions across different bootstrap samples.\n",
        "\n",
        "- Summary: Random Forest produces more robust and less noisy feature importance estimates than a single decision tree.\n",
        "\n",
        "\n",
        "### Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "### Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "### Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "● Print the best parameters and final accuracy\n",
        "\n",
        "### Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "### Question 10: You are working as a data scientist at a financial institution to predict loandefault. You have access to customer demographic and transaction history data.You decide to use ensemble techniques to increase model performance.Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "\n",
        "● Handle overfitting\n",
        "\n",
        "● Select base models\n",
        "\n",
        "● Evaluate performance using cross-validation\n",
        "\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "- Scenario: Predict loan default using customer demographic and transaction history. You plan to use ensemble methods to improve performance.\n",
        "\n",
        "- Step-by-step approach:\n",
        "  - 1. Understand data & problem\n",
        "     - Target: default (binary) — classification problem.\n",
        "     - Features: demographics (age, income, employment), transactions (balances, payments), credit history, behavioral signals.\n",
        "     - Check class balance (defaults may be minority).\n",
        " - 2. Preprocessing & feature engineering\n",
        "     - Data cleaning: missing values, outliers.\n",
        "     - Feature engineering: calculate credit utilization ratio, transaction frequency, trend features (last 3 months), aggregation features.\n",
        "     - Encoding categorical variables (one-hot, target encoding if many categories).\n",
        "     - Scaling numeric features if using distance-based learners.\n",
        " - 3. Choose between Bagging or Boosting\n",
        "     - If model variance is high (overfitting) and base learner is unstable (e.g., deep trees), Bagging / Random Forest can help reduce variance.\n",
        "     - If bias is high (underfitting) and you need high predictive accuracy, Boosting (XGBoost / LightGBM / CatBoost) is often better — especially with tabular structured data.\n",
        "     - For loan default, boosting methods (LightGBM/XGBoost/CatBoost) are often chosen in practice for high accuracy, but RandomForest is a strong baseline and more tolerant to noise.\n",
        " - 4. Handle class imbalance\n",
        "   - Techniques:\n",
        "     - Resampling: SMOTE (synthetic oversampling) or ADASYN.\n",
        "     - Use class_weight='balanced' in models.\n",
        "     - Use evaluation metrics robust to imbalance: AUC-ROC, Precision-Recall AUC, F1, recall at specific thresholds.\n",
        "    - Prefer combining methods (e.g., boosting + class weighting) if needed.\n",
        "   \n",
        "  - 5. Select base models\n",
        "    - For Boosting: LightGBM / XGBoost / CatBoost.\n",
        "    - For Bagging: RandomForest or Bagging with DecisionTree base.\n",
        "    - Also try stacking: combine multiple diverse models (e.g., LogisticRegression, RandomForest, XGBoost) and train a meta-learner (often a simple model like Logistic Regression) on their predictions.\n",
        "  - 6. Hyperparameter tuning\n",
        "    - Use GridSearchCV / RandomizedSearchCV or specialized libraries (Optuna) with cross-validation (e.g., stratified K-fold).\n",
        "    - For time-sensitive data, use TimeSeriesSplit or careful validation scheme.\n",
        "  - 7. Evaluate with cross-validation\n",
        "    - Use Stratified K-Fold CV to preserve class ratio.\n",
        "    - Track metrics: ROC-AUC, Precision-Recall AUC, F1, also confusion matrix at chosen threshold.\n",
        "    - Use OOB score for RandomForest as additional check.\n",
        "\n",
        "  - 8. Address overfitting\n",
        "    - Regularize models (learning rate, max_depth, min_child_weight).\n",
        "    - Early stopping for boosting (monitor validation AUC).\n",
        "    - Use simpler model as baseline and feature selection to remove noisy features.\n",
        "  - 9. Model interpretability & fairness\n",
        "    - Use SHAP or LIME for local/global explainability — important in finance to justify decisions.\n",
        "    - Check for bias across demographic groups (fairness audits).\n",
        "  - 10. Deployment & monitoring\n",
        "    - Deploy model as a service; implement prediction pipelines.\n",
        "    - Monitor performance drift: population shift, concept drift — retrain periodically.\n",
        "    - Keep human-in-the-loop: manual review for high-impact decisions.\n",
        "- Why ensemble learning improves decision-making (real-world):\n",
        "   - Higher predictive accuracy: Boosting often yields state-of-the-art performance on tabular data.\n",
        "   - Robustness: Bagging reduces variance; ensemble is less sensitive to noise/outliers.\n",
        "   - Better generalization: Combing multiple learners reduces the chance of a single-model failure.\n",
        "   - Explainability options: Techniques like SHAP work on ensembles, allowing feature-level explanations necessary in regulated domains like finance.\n",
        "   - Risk control: Better predictions help reduce credit risk and losses; accurate ranking of applicants helps prioritize manual review."
      ],
      "metadata": {
        "id": "EI4bQ333S2x8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WuZHsaRS1wy",
        "outputId": "a5f56733-bcf4-4924-c53e-e05c3cc3ff34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Solution 06\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Feature importances\n",
        "importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "print(\"Top 5 features:\")\n",
        "print(importances.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 07\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Single decision tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Bagging with decision trees\n",
        "bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "print(f\"Decision Tree accuracy: {dt_acc:.4f}\")\n",
        "print(f\"Bagging (DecisionTree base) accuracy: {bag_acc:.4f}\")"
      ],
      "metadata": {
        "id": "EJhN4R98VWum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 08\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 3, 5, 10]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X, y)\n",
        "\n",
        "print(\"Best parameters:\", grid.best_params_)\n",
        "print(\"Best CV score:\", grid.best_score_)\n",
        "\n",
        "# evaluate best estimator on hold-out split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
        "best_rf = grid.best_estimator_\n",
        "best_rf.fit(X_train, y_train)\n",
        "print(\"Final holdout accuracy:\", accuracy_score(y_test, best_rf.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YFkFFR5Vh6I",
        "outputId": "432fcf4f-06d8-47e8-9709-270d3d55f2e0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': None, 'n_estimators': 50}\n",
            "Best CV score: 0.9666666666666668\n",
            "Final holdout accuracy: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 09\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "cal = fetch_california_housing()\n",
        "X = cal.data\n",
        "y = cal.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bagging with DecisionTreeRegressor\n",
        "bag_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bag_reg.fit(X_train, y_train)\n",
        "pred_bag = bag_reg.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, pred_bag)\n",
        "\n",
        "# RandomForestRegressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, pred_rf)\n",
        "\n",
        "print(f\"BaggingRegressor MSE: {mse_bag:.4f}\")\n",
        "print(f\"RandomForestRegressor MSE: {mse_rf:.4f}\")"
      ],
      "metadata": {
        "id": "VJgf1wRXVmso"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}